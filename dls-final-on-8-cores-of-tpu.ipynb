{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n!pip uninstall -y torch\n!pip install torch==1.9\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport numpy as np\nfrom torch.autograd import Variable\nfrom torchvision import datasets\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch\n\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\n\nimport torchvision\nfrom torchvision import transforms\n# os.environ['XLA_USE_BF16'] = '1' #Setting this Environment variable allows TPU to use 'bfloat16'\n","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"id":"WJyXRyZPqwrt","outputId":"7bdc1f86-3a2a-4b93-f1ca-dfff8ee8cad1","execution":{"iopub.status.busy":"2022-02-06T16:05:21.77017Z","iopub.execute_input":"2022-02-06T16:05:21.770851Z","iopub.status.idle":"2022-02-06T16:07:38.217165Z","shell.execute_reply.started":"2022-02-06T16:05:21.770747Z","shell.execute_reply":"2022-02-06T16:07:38.215994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import multiprocessing\nimport torch_xla.debug.profiler as xp\nimport torch_xla.utils.utils as xu","metadata":{"execution":{"iopub.status.busy":"2022-02-06T16:07:38.220153Z","iopub.execute_input":"2022-02-06T16:07:38.22088Z","iopub.status.idle":"2022-02-06T16:07:38.227196Z","shell.execute_reply.started":"2022-02-06T16:07:38.220829Z","shell.execute_reply":"2022-02-06T16:07:38.226145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\nimport cv2\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as tt\nfrom torchvision.utils import make_grid\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom IPython.display import clear_output\nimport warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"id":"tnc-g3_xqwrp","execution":{"iopub.status.busy":"2022-02-06T16:07:38.228495Z","iopub.execute_input":"2022-02-06T16:07:38.228825Z","iopub.status.idle":"2022-02-06T16:07:38.427262Z","shell.execute_reply.started":"2022-02-06T16:07:38.228792Z","shell.execute_reply":"2022-02-06T16:07:38.426189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"fpb_LlC0raeH","outputId":"e2a960c0-8919-466b-b49a-546c0734622e","execution":{"iopub.status.busy":"2022-02-06T16:07:38.429634Z","iopub.execute_input":"2022-02-06T16:07:38.429962Z","iopub.status.idle":"2022-02-06T16:07:38.434506Z","shell.execute_reply.started":"2022-02-06T16:07:38.429929Z","shell.execute_reply":"2022-02-06T16:07:38.433294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageDS(Dataset):\n    \n    def __init__(self, path_a, path_b, image_size):\n        super().__init__()\n        self.image_size = image_size\n        self.images_a = self.process_folder(path_a, crop='cartoon')\n        self.images_b = self.process_folder(path_b, crop='face')\n        self.size_a = len(self.images_a)\n        self.size_b = len(self.images_b)\n        self.bigger = True if self.size_a >= self.size_b else False\n        \n    def process_folder(self, path, crop):\n        images = []\n#         file_names = os.listdir(path)\n#         if DATASET_SIZE != -1:\n#             file_names = file_names[:DATASET_SIZE]\n#         for fname in tqdm(file_names):\n        progress_bar = tqdm(total=DATASET_SIZE, desc='Images processed:', position=0)      \n        for dirname, _, filenames in os.walk(path):\n            if len(images) >= DATASET_SIZE:\n                    break\n            for filename in filenames:\n                if len(images) >= DATASET_SIZE:\n                    break\n                if filename.endswith('.jpg') or filename.endswith('.png'):\n            #                 f_path = path + fname\n            #                 image = Image.open(f_path)\n            #                 if image.getbands() == ('R', 'G', 'B'):\n                    image = Image.open(os.path.join(dirname, filename))\n                    progress_bar.update(1)\n                    scale = 1.5 if crop == 'cartoon' else 1.0\n                    transform = tt.Compose([\n                            tt.Resize(int(self.image_size * scale)),\n                            tt.CenterCrop(self.image_size),\n                            tt.RandomHorizontalFlip(),\n                            tt.ToTensor(),\n                            tt.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n                    image = transform(image)\n                    images.append(image)\n                    \n#                     clear_output()\n#                     print(len(images), '/', DATASET_SIZE, sep='')\n                    if len(images) >= DATASET_SIZE:\n                        break\n                \n        return images\n        \n    \n    def __len__(self):\n        return self.size_a if self.bigger else self.size_b\n    \n    def __getitem__(self, index):\n        if self.bigger:\n            image_a = self.images_a[index]\n            image_b = self.images_b[int(index * (self.size_b / self.size_a))]\n        else:\n            image_a =  self.images_a[int(index * (self.size_a / self.size_b))]\n            image_b =  self.images_b[index]\n            \n        return image_a, image_b\n# # Apples2oranges\n# path_tr_a = '../input/apple2orange-dataset/trainA/'\n# path_tr_b ='../input/apple2orange-dataset/trainB/'\n# path_tst_a = '../input/apple2orange-dataset/testA/'\n# path_tst_b = '../input/apple2orange-dataset/testB/'\n\n# # Monet2photo\n# path_tr_a = '../input/monet2photo/trainA/'\n# path_tr_b ='../input/monet2photo/trainB/'\n# path_tst_a = '../input/monet2photo/testA/'\n# path_tst_b = '../input/monet2photo/testB/'\n\n# cars2toys\n# path_tr_a = '../input/stanford-cars-dataset/cars_train/cars_train/'\n# path_tr_b = '../input/toy-cars-annotated-on-yolo-format/'\n\n# faces2cartoons\npath_tr_a = '../input/cartoon-faces-googles-cartoon-set/'\npath_tr_b ='../input/flickrfaces-dataset-nvidia-128x128/real_faces_128/'\n# path_tr_a = '/content/cartoonset100k_jpg'\n# path_tr_b ='/content/real_faces_128'\n\n\nIMAGE_SIZE = 64\nBATCH_SIZE = 10\nDATASET_SIZE = 320\n\n# train_ds = ImageDS(path_tr_a, path_tr_b, IMAGE_SIZE)\n# test_ds = ImageDS(path_tst_a, path_tst_b, IMAGE_SIZE)\n\n\n\n# train_sampler = torch.utils.data.distributed.DistributedSampler(\n#           train_ds,\n#           num_replicas=xm.xrt_world_size(),\n#           rank=xm.get_ordinal(),\n#           shuffle=True)\n\n# test_sampler = torch.utils.data.distributed.DistributedSampler(\n#           test_ds,\n#           num_replicas=xm.xrt_world_size(),\n#           rank=xm.get_ordinal(),\n#           shuffle=False)\n\n# train_dl = DataLoader(train_ds, BATCH_SIZE, sampler=train_sampler, num_workers=4, pin_memory=True)\n# test_dl = DataLoader(test_ds, BATCH_SIZE, sampler=test_sampler,num_workers=4, pin_memory=True)\n\n","metadata":{"id":"AlTHKjGkqwry","outputId":"585e38a7-34a3-4569-ce32-9cd3f4c93a77","execution":{"iopub.status.busy":"2022-02-06T16:07:38.436798Z","iopub.execute_input":"2022-02-06T16:07:38.437214Z","iopub.status.idle":"2022-02-06T16:07:38.459274Z","shell.execute_reply.started":"2022-02-06T16:07:38.437159Z","shell.execute_reply":"2022-02-06T16:07:38.458457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def denorm(img_tensors):\n    return img_tensors * 0.5 + 0.5\n\ndef reduce(values):\n    '''    \n    Returns the average of the values.\n    Args:\n        values : list of any value which is calulated on each core \n    '''\n    return sum(values) / len(values)\n\ndef show(a, b, c):\n    plt.figure(figsize=(5,2))\n    plt.imshow(denorm(make_grid(torch.cat((a[:1], b[:1], c[:1]), dim=0), nrow=3).permute(1,2,0)))\n    plt.show()\n# images_a = torch.empty(0,3,image_size, image_size)\n# images_b = torch.empty(0,3,image_size, image_size)\n# for _ in range(12):\n#     image_a, image_b = next(iter(train_dl))\n#     images_a = torch.vstack((images_a, image_a))\n#     images_b = torch.vstack((images_b, image_b))\na, b = next(iter(train_dl))\nc = a\nshow(a, b, c)","metadata":{"id":"zsspCqi1qwr0","outputId":"24d43d2a-0ea6-4a86-f581-927920ef9c17","execution":{"iopub.status.busy":"2022-02-06T16:07:38.460859Z","iopub.execute_input":"2022-02-06T16:07:38.461883Z","iopub.status.idle":"2022-02-06T16:07:38.602954Z","shell.execute_reply.started":"2022-02-06T16:07:38.461813Z","shell.execute_reply":"2022-02-06T16:07:38.60136Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def conv_block(in_ch, out_ch, kernel=3, stride=2, padding=1, normalize=True, activation=nn.LeakyReLU(0.2, inplace=True)):\n    layers = [nn.ReflectionPad2d(padding)] if padding > 0 else []\n    layers += [nn.Conv2d(in_ch, out_ch, kernel_size=kernel, stride=stride, padding=0)]\n    if normalize:\n        layers += [nn.BatchNorm2d(out_ch, affine=True) if BATCH_SIZE > 1 else nn.InstanceNorm2d(out_ch, affine=True)]\n    if activation:\n        layers += [activation]\n    return layers\n\ndef deconv_block(in_ch, out_ch,\n                 kernel=3, stride=1, padding=1,\n                 normalize=True,\n                 activation=True):\n    layers = [nn.Upsample(scale_factor = 2, mode='bilinear'),\n              nn.ReflectionPad2d(padding),\n              nn.Conv2d(in_ch, out_ch, kernel_size=kernel, stride=stride, padding=0)]\n    if normalize:\n        layers += [nn.BatchNorm2d(out_ch, affine=True) if BATCH_SIZE > 1 else nn.InstanceNorm2d(out_ch, affine=True)]\n    if activation:\n        layers += [nn.ReLU(inplace=True)]\n    return layers\n    \nclass Res_block(nn.Module):\n    \n    def __init__(self, in_ch):\n        super().__init__()\n        self.block = nn.Sequential(nn.ReflectionPad2d(1),\n                                   nn.Conv2d(in_ch, in_ch, kernel_size=3),\n                                   nn.BatchNorm2d(in_ch, affine=True) if BATCH_SIZE > 1 else nn.InstanceNorm2d(in_ch, affine=True),\n                                   nn.ReLU(inplace=True),\n                                   nn.ReflectionPad2d(1),\n                                   nn.Conv2d(in_ch, in_ch, kernel_size=3),\n                                   nn.BatchNorm2d(in_ch, affine=True) if BATCH_SIZE > 1 else nn.InstanceNorm2d(in_ch, affine=True)\n                                  )\n    def forward(self, x):\n        return x + self.block(x)\n    \n    \nclass Discriminator(nn.Module):\n    \n    def __init__(self, in_ch=3, features=[64, 128, 256, 512]):\n        super().__init__()\n        layers = []\n        for out_ch in features:\n            layers += conv_block(in_ch, out_ch, kernel=4)\n            in_ch = out_ch\n        layers += conv_block(features[-1], 1, kernel=4, stride=1, padding=0, normalize=False, activation=nn.Sigmoid())\n        self.conv = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        return self.conv(x).view(-1, 1)\n    \nclass Generator(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        layers = []\n        layers += conv_block(3, 64, kernel=5, stride=1, padding=2, activation=nn.ReLU(inplace=True))\n        layers += conv_block(64,128, activation=nn.ReLU(inplace=True))\n        layers += conv_block(128,256, activation=nn.ReLU(inplace=True))\n        layers += [Res_block(256)] * 3\n        layers += deconv_block(256, 128, activation=nn.ReLU(inplace=True))\n        layers += deconv_block(128, 64, activation=nn.ReLU(inplace=True))\n        layers += conv_block(64, 3, kernel=5, stride=1, padding=2, normalize=False, activation=nn.Tanh())\n        self.net = nn.Sequential(*layers)\n        \n    def forward(self, x):\n        return self.net(x)\n        \ndef test():\n    D = Discriminator()\n    G = Generator()\n    test = torch.randn(BATCH_SIZE, 3, IMAGE_SIZE, IMAGE_SIZE)\n    print(D(test).shape)\n    print(G(test).shape)\n    print(D)\n    print(G)\n    \ntest()\n","metadata":{"id":"kNQdD0B0qwr0","outputId":"81dddb29-a386-43cd-d938-3c5f89a37c5b","execution":{"iopub.status.busy":"2022-02-06T16:08:05.704861Z","iopub.execute_input":"2022-02-06T16:08:05.70521Z","iopub.status.idle":"2022-02-06T16:08:06.552597Z","shell.execute_reply.started":"2022-02-06T16:08:05.705173Z","shell.execute_reply":"2022-02-06T16:08:06.551561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\nimport random\n\nclass DecayLR:\n    def __init__(self, epochs, offset, decay_epochs):\n        epoch_flag = epochs - decay_epochs\n        assert (epoch_flag > 0), \"Decay must start before the training session ends!\"\n        self.epochs = epochs\n        self.offset = offset\n        self.decay_epochs = decay_epochs\n\n    def step(self, epoch):\n        return 1.0 - max(0, epoch + self.offset - self.decay_epochs) / (\n                self.epochs - self.decay_epochs)\n    \nclass ReplayBuffer:\n    def __init__(self, max_size=50):\n        assert (max_size > 0), \"Empty buffer or trying to create a black hole. Be careful.\"\n        self.max_size = max_size\n        self.data = []\n\n    def push_and_pop(self, data):\n        to_return = []\n        for element in data.data:\n            element = torch.unsqueeze(element, 0)\n            if len(self.data) < self.max_size:\n                self.data.append(element)\n                to_return.append(element)\n            else:\n                if random.uniform(0, 1) > 0.5:\n                    i = random.randint(0, self.max_size - 1)\n                    to_return.append(self.data[i].clone())\n                    self.data[i] = element\n                else:\n                    to_return.append(element)\n        return torch.cat(to_return)\n\n\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    torch.manual_seed(1)\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    elif classname.find(\"BatchNorm\") != -1:\n        torch.nn.init.normal_(m.weight, 1.0, 0.02)\n        torch.nn.init.zeros_(m.bias)","metadata":{"id":"01VtI3PRqwr1","execution":{"iopub.status.busy":"2022-02-06T16:08:06.554594Z","iopub.execute_input":"2022-02-06T16:08:06.554889Z","iopub.status.idle":"2022-02-06T16:08:06.568835Z","shell.execute_reply.started":"2022-02-06T16:08:06.554858Z","shell.execute_reply":"2022-02-06T16:08:06.567704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_one_epoch(train_dl,\n                    netG_A2B, netG_B2A, netD_A, netD_B,\n                    cycle_loss, identity_loss, adversarial_loss,\n                    optimizer_G,optimizer_D_A, optimizer_D_B,\n                    epoch, epochs,\n                    device, rank\n                   ):\n#     i=0\n#     progress_bar = tqdm(total=len(train_dl), desc=f'Core: {rank}, batch: {i+1}', position=0)  \n    for batch_num, data in enumerate(train_dl):\n        print(f'epoch: {epoch} core: {rank} batch: {batch_num}')\n        # get batch size data\n        real_image_A = data[0].to(device)\n        real_image_B = data[1].to(device)\n        batch_size = real_image_A.size(0)\n        if rank == 0:\n            print('1')\n        # real data label is 1, fake data label is 0.\n        real_label = torch.full((batch_size, 1), 1, dtype=torch.float32).uniform_(0.8, 1.1).float().to(device)\n        fake_label = torch.full((batch_size, 1), 0, dtype=torch.float32).uniform_(0., 0.3).float().to(device)\n        ##############################################\n        # (1) Update G network: Generators A2B and B2A\n        ##############################################\n        # Set G_A and G_B's gradients to zero\n        optimizer_G.zero_grad()\n        if rank == 0:\n            print('2')\n        # Identity loss\n        # G_B2A(A) should equal A if real A is fed\n        identity_image_A = netG_B2A(real_image_A)\n        loss_identity_A = identity_loss(identity_image_A, real_image_A) * 2.0\n        if rank == 0:\n            print('3')\n        # G_A2B(B) should equal B if real B is fed\n        identity_image_B = netG_A2B(real_image_B)\n        loss_identity_B = identity_loss(identity_image_B, real_image_B) * 2.0\n        # GAN loss\n        # GAN loss D_A(G_A(A))\n        fake_image_A = netG_B2A(real_image_B)\n        fake_output_A = netD_A(fake_image_A)\n        loss_GAN_B2A = adversarial_loss(fake_output_A, real_label)\n        # GAN loss D_B(G_B(B))\n        fake_image_B = netG_A2B(real_image_A)\n        fake_output_B = netD_B(fake_image_B)\n        loss_GAN_A2B = adversarial_loss(fake_output_B, real_label)\n        # Cycle loss\n        recovered_image_A = netG_B2A(fake_image_B)\n        loss_cycle_ABA = cycle_loss(recovered_image_A, real_image_A) * 3.0\n\n        recovered_image_B = netG_A2B(fake_image_A)\n        loss_cycle_BAB = cycle_loss(recovered_image_B, real_image_B) * 3.0\n\n        # Combined loss and calculate gradients\n        errG = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_ABA + loss_cycle_BAB\n        # Calculate gradients for G_A and G_B\n        errG.backward()\n        if rank == 0:\n            print('4')\n        # Update G_A and G_B's weights\n        xm.optimizer_step(optimizer_G)\n        if rank == 0:\n            print('5')\n\n        ##############################################\n        # (2) Update D network: Discriminator A\n        ##############################################\n\n        # Set D_A gradients to zero\n        optimizer_D_A.zero_grad()\n\n        # Real A image loss\n        real_output_A = netD_A(real_image_A)\n        errD_real_A = adversarial_loss(real_output_A, real_label)\n\n        # Fake A image loss\n#         fake_image_A = fake_A_buffer.push_and_pop(fake_image_A)\n        fake_output_A = netD_A(fake_image_A.detach())\n        errD_fake_A = adversarial_loss(fake_output_A, fake_label)\n\n        # Combined loss and calculate gradients\n        errD_A = (errD_real_A + errD_fake_A) / 2\n\n        # Calculate gradients for D_A\n        errD_A.backward()\n        # Update D_A weights\n        xm.optimizer_step(optimizer_D_A)\n\n\n        ##############################################\n        # (3) Update D network: Discriminator B\n        ##############################################\n\n        # Set D_B gradients to zero\n        optimizer_D_B.zero_grad()\n\n        # Real B image loss\n        real_output_B = netD_B(real_image_B)\n        errD_real_B = adversarial_loss(real_output_B, real_label)\n\n        # Fake B image loss\n#         fake_image_B = fake_B_buffer.push_and_pop(fake_image_B)\n        fake_output_B = netD_B(fake_image_B.detach())\n        errD_fake_B = adversarial_loss(fake_output_B, fake_label)\n\n        # Combined loss and calculate gradients\n        errD_B = (errD_real_B + errD_fake_B) / 2\n\n        # Calculate gradients for D_B\n        errD_B.backward()\n        # Update D_B weights\n        xm.optimizer_step(optimizer_D_B)\n#             progress_bar.set_description(\n#                 f\"[{epoch}/{epochs - 1}][{i}/{len(train_dl) - 1}] \"\n#                 f\"Loss_D: {(errD_A + errD_B).item():.4f} \"\n#                 f\"Loss_G: {errG.item():.4f} \"\n#                 f\"Loss_G_identity: {(loss_identity_A + loss_identity_B).item():.4f} \"\n#                 f\"loss_G_GAN: {(loss_GAN_A2B + loss_GAN_B2A).item():.4f} \"\n#                 f\"loss_G_cycle: {(loss_cycle_ABA + loss_cycle_BAB).item():.4f}\")\n\n#         i+=1\n#         progress_bar.update(1)\n#         if rank == 0 and (batch_num+1) % 2 == 0:\n#             iterator = iter(train_dl)\n#             images_a, images_b = next(iterator)\n#             images_a_real = images_a.to(device)[:1]\n#             images_b_real = images_b.to(device)[:1]\n#             netG_A2B.eval()\n#             netG_B2A.eval()\n#             netD_A.eval()\n#             netD_B.eval()\n#             with torch.no_grad():\n#                 a2b_transformed = netG_A2B(images_a_real)\n#                 b2a_transformed = netG_B2A(images_b_real)\n#                 a2b2a_recovered = netG_B2A(a2b_transformed)\n#                 b2a2b_recovered = netG_A2B(b2a_transformed)\n#             plt.figure(figsize=(17,4))\n#             grid = plt.GridSpec(4, 2, wspace = .1, hspace = .35)\n#             plt.subplot(grid[0:2, 0:2])\n#             plt.imshow(denorm(make_grid(torch.cat((images_a_real.detach().cpu(),\n#                                                    a2b_transformed.detach().cpu(),\n#                                                    a2b2a_recovered.detach().cpu()), dim=0), nrow=6\n#                                        ).permute(1,2,0)\n#                              )\n#                       )\n#             plt.axis('off')\n#             plt.subplot(grid[0:2, 2:4])\n#             plt.imshow(denorm(make_grid(torch.cat((images_b_real.detach().cpu(),\n#                                                    b2a_transformed.detach().cpu(),\n#                                                    b2a2b_recovered.detach().cpu()), dim=0), nrow=6\n#                                        ).permute(1,2,0)\n#                              )\n#                       )\n#             plt.axis('off')\n# # #             plt.subplot(grid[2:4, 0:4])\n# # #             plt.plot(g_losses, label=\"G\")\n# # #             plt.plot(d_losses, label=\"D\")\n# # #             plt.legend()\n# #             plt.suptitle(t=f'Batch {i+1}/{len(train_dl)/BATCH_SIZE}:',\n# #                       y=0.95, x=0.5)\n#             plt.show()\n#             netG_A2B.train()\n#             netG_B2A.train()\n#             netD_A.train()\n#             netD_B.train()\n            \n    # Update learning rates\n    # lr_scheduler_G.step()\n    # lr_scheduler_D_A.step()\n    # lr_scheduler_D_B.step()","metadata":{"id":"y8O2kPT2qwr2","execution":{"iopub.status.busy":"2022-02-06T16:19:55.903509Z","iopub.execute_input":"2022-02-06T16:19:55.904236Z","iopub.status.idle":"2022-02-06T16:19:55.932066Z","shell.execute_reply.started":"2022-02-06T16:19:55.904175Z","shell.execute_reply":"2022-02-06T16:19:55.930882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SERIAL_EXEC = xmp.MpSerialExecutor()\n# Only instantiate model weights once in memory.\nWRAPPED_D_A = xmp.MpModelWrapper(Discriminator())\nWRAPPED_D_B = xmp.MpModelWrapper(Discriminator())\nWRAPPED_G_A2B = xmp.MpModelWrapper(Generator())\nWRAPPED_G_B2A = xmp.MpModelWrapper(Generator())\n\ndef get_dataset():\n    path_tr_a = '../input/cartoon-faces-googles-cartoon-set/'\n    path_tr_b ='../input/flickrfaces-dataset-nvidia-128x128/real_faces_128/'\n    train_ds = ImageDS(path_tr_a, path_tr_b, IMAGE_SIZE)\n    return train_ds\ndef _mp_fn(rank,flags):\n    '''\n    This function is executed on all the devices when it is spawned.\n    Args :\n        rank  - Index of the process.\n        flags - Arguments you need to pass to each process.\n    '''\n    torch.manual_seed(1)\n    torch.set_default_tensor_type('torch.FloatTensor')\n    device = xm.xla_device()\n    \n    train_ds = SERIAL_EXEC.run(get_dataset)\n    data_sampler = DistributedSampler(train_ds,\n                                      num_replicas=xm.xrt_world_size(),\n                                      rank=xm.get_ordinal(),\n                                      shuffle=True)\n    train_dl = DataLoader(\n                          train_ds,\n                          batch_size=flags['batch_size'],\n                          sampler=data_sampler,\n                          num_workers=flags['num_workers'],\n                          drop_last=True)\n    \n\n#     netG_A2B.apply(weights_init)\n#     netG_B2A.apply(weights_init)\n#     netD_A.apply(weights_init)\n#     netD_B.apply(weights_init)\n\n#     netG_A2B.load_state_dict(torch.load('../input/41-epochs-faces/5000_G_A2B'))\n#     netG_B2A.load_state_dict(torch.load('../input/41-epochs-faces/5000_G_B2A'))\n    # netD_A.load_state_dict(torch.load('/content/5000_D_A'))\n    # netD_B.load_state_dict(torch.load('/content/5000_D_B'))\n    netG_A2B = WRAPPED_G_A2B.to(device)\n    netG_B2A = WRAPPED_G_B2A.to(device)\n    netD_A = WRAPPED_D_A.to(device)\n    netD_B = WRAPPED_D_B.to(device)\n    optimizer_G = torch.optim.Adam(itertools.chain(netG_A2B.parameters(), netG_B2A.parameters()),\n                                   lr=flags['lr'], betas=(0.5, 0.999))\n    optimizer_D_A = torch.optim.Adam(netD_A.parameters(), lr=flags['lr'], betas=(0.5, 0.999))\n    optimizer_D_B = torch.optim.Adam(netD_B.parameters(), lr=flags['lr'], betas=(0.5, 0.999))\n    cycle_loss = torch.nn.L1Loss()\n    identity_loss = torch.nn.L1Loss()\n    adversarial_loss = torch.nn.MSELoss()\n    lr=0.0002\n    epochs = 1\n    decay_epochs=100\n    xm.master_print('Training has started\\n')\n    for epoch in range(flags['nb_epochs']):\n        xm.master_print(f'Epoch[{epoch+1}/{flags[\"nb_epochs\"]}] has started\\n')\n        parallel_loader = pl.ParallelLoader(train_dl, [device]).per_device_loader(device)\n        train_one_epoch(parallel_loader,\n                        netG_A2B, netG_B2A, netD_A, netD_B,\n                        cycle_loss, identity_loss, adversarial_loss,\n                        optimizer_G,optimizer_D_A, optimizer_D_B, epoch, flags['nb_epochs'],\n                        device, rank)\n        xm.master_print(f'Epoch[{epoch+1}/{flags[\"nb_epochs\"]}] has completed\\n')\n#         del parallel_loader\n#         gc.collect()","metadata":{"id":"GY-DnocSqwr4","execution":{"iopub.status.busy":"2022-02-06T16:19:55.933957Z","iopub.execute_input":"2022-02-06T16:19:55.934819Z","iopub.status.idle":"2022-02-06T16:19:56.081781Z","shell.execute_reply.started":"2022-02-06T16:19:55.934766Z","shell.execute_reply":"2022-02-06T16:19:56.080781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# applying multiprocessing so that images get trained different on      # cores of kaggle-tpuG_A2B = Generator().float()G_B2A = Generator().float()\n\nflags = {\n         'batch_size': 10,\n         'lr':0.0016,\n         'nb_epochs':5,\n         'num_workers': 8\n        }     \nxmp.spawn(_mp_fn, args=(flags,), nprocs=8, start_method='fork')","metadata":{"id":"obq9ThVCqwr6","outputId":"213e7f81-0c65-4dd9-84c3-1488c6e40117","execution":{"iopub.status.busy":"2022-02-06T16:19:56.083346Z","iopub.execute_input":"2022-02-06T16:19:56.084082Z","iopub.status.idle":"2022-02-06T17:04:47.120569Z","shell.execute_reply.started":"2022-02-06T16:19:56.084031Z","shell.execute_reply":"2022-02-06T17:04:47.118875Z"},"trusted":true},"execution_count":null,"outputs":[]}]}